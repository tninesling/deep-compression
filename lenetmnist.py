# -*- coding: utf-8 -*-
"""LeNetMNIST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_B3vXnQMHsrA4vFvytrQgWeLFdKVrJuS
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import torch.quantization
import torch.nn.utils.prune as prune
import scipy.sparse as sp
from torch.ao.quantization import QuantStub, DeQuantStub
import os

#LeNet5 network
class LeNet5(nn.Module):
    def __init__(self, q):
        self.q = q
        super(LeNet5, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc1 = nn.Linear(16 * 4 * 4, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)
        if self.q:
          self.quant = QuantStub()
          self.dequant = DeQuantStub()

    def forward(self, x):
        if self.q:
          x = self.quant(x)
        x = torch.relu(self.conv1(x))
        x = torch.max_pool2d(x, 2)
        x = torch.relu(self.conv2(x))
        x = torch.max_pool2d(x, 2)
        x = x.reshape(-1, 16 * 4 * 4)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        if self.q:
          x = self.dequant(x)
        return x

    def train(self):
      criterion = nn.CrossEntropyLoss()
      optimizer = optim.Adam(self.parameters(), lr=0.001)

      # Train the models
      for epoch in range(5):
          for i, (images, labels) in enumerate(train_loader):
              optimizer.zero_grad()
              outputs = self(images)
              loss = criterion(outputs, labels)
              loss.backward()
              optimizer.step()

    def evaluate(self):
      self.correct = 0
      self.total = 0
      self.eval()
      with torch.no_grad():
          for inputs, labels in test_loader:
              outputs = self(inputs)
              _, predicted = torch.max(outputs.data, 1)
              self.total += labels.size(0)
              self.correct += (predicted == labels).sum().item()
      return (100 * (1 - (self.correct / self.total)))

#LeNet300 network
class LeNet300(nn.Module):
    def __init__(self, q):
        self.q = q
        super(LeNet300, self).__init__()
        self.fc1 = nn.Linear(28*28, 300)
        self.fc2 = nn.Linear(300, 100)
        self.fc3 = nn.Linear(100, 10)
        self.relu = nn.ReLU()
        if self.q:
          self.quant = QuantStub()
          self.dequant = DeQuantStub()

    def forward(self, x):
        if self.q:
          x = self.quant(x)
        x = x.reshape(-1, 28*28)  # Flatten the input images
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        if self.q:
          x = self.dequant(x)
        return x

    def train(self):
      criterion = nn.CrossEntropyLoss()
      optimizer = optim.Adam(self.parameters(), lr=0.001)
      total_step = len(train_loader)
      for epoch in range(10):
          for i, (images, labels) in enumerate(train_loader):
              outputs = self(images)
              loss = criterion(outputs, labels)
              optimizer.zero_grad()
              loss.backward()
              optimizer.step()

    def evaluate(self):
      self.correct = 0
      self.total = 0
      with torch.no_grad():
          for inputs, labels in test_loader:
              outputs = self(inputs)
              _, predicted = torch.max(outputs.data, 1)
              self.total += labels.size(0)
              self.correct += (predicted == labels).sum().item()
      return (100 * (1 - (self.correct / self.total)))

#get dataset and normalize
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])
train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)

#initialize models
# model5 = LeNet5(False)
model300 = LeNet300(False)

#train models
# model5.train()
model300.train()

#save trained models
# torch.save(model5.state_dict(), "trainedModel5.p")
torch.save(model300.state_dict(), "trainedModel300.p")

#print model size without quantization
# print('Size no quantization LeNet5 (MB):', os.path.getsize("noquant.p")/1e6)

if __name__ == "__main__":
  print('Size no pruning LeNet300 (MB):', os.path.getsize("trainedModel300noquant.p")/1e6)
  print('Top1 error on LeNet300 no prune: %f %%' % (model300.evaluate()))


  #########L1 unstructured pruning##############

  model300l1u= LeNet300(False)
  model300l1u.load_state_dict(torch.load("trainedModel300.p"))


  for name, module in model300l1u.named_modules():
    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):
      prune.l1_unstructured(module, name="weight", amount=0.1)
      prune.remove(module, 'weight')

  torch.save(model300l1u.state_dict(), "quant.p")
  print('Size after l1 unstructured pruning LeNet300 (MB):', os.path.getsize("quant.p")/1e6)
  os.remove('quant.p')

  print('Top1 error on LeNet300 l1 unstructured: %f %%' % (model300l1u.evaluate()))

  ##########random unstructured pruning############

  model300ru = LeNet300(False)
  model300ru.load_state_dict(torch.load("trainedModel300.p"))

  for name, module in model300ru.named_modules():
    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):
      prune.random_unstructured(module, name="weight", amount=0.1)
      prune.remove(module, 'weight')

  torch.save(model300ru.state_dict(), "quant.p")
  print('Size after random unstructured pruning LeNet300 (MB):', os.path.getsize("quant.p")/1e6)
  os.remove('quant.p')

  print('Top1 error on LeNet300 random unstructured: %f %%' % (model300ru.evaluate()))

  ##########ln structured pruning############

  model300lns = LeNet300(False)
  model300lns.load_state_dict(torch.load("trainedModel300.p"))

  for name, module in model300lns.named_modules():
    if isinstance(module, torch.nn.Conv2d):
      prune.ln_structured(module, name="weight", amount=0.1, n=2, dim=0)
      prune.remove(module, 'weight')

  torch.save(model300lns.state_dict(), "quant.p")
  print('Size after ln structured pruning LeNet300 (MB):', os.path.getsize("quant.p")/1e6)
  os.remove('quant.p')

  print('Top1 error on LeNet300 ln structured: %f %%' % (model300lns.evaluate()))

  ##########random structured pruning############

  model300rs = LeNet300(False)
  model300rs.load_state_dict(torch.load("trainedModel300.p"))

  for name, module in model300rs.named_modules():
    if isinstance(module, torch.nn.Conv2d):
      prune.random_structured(module, name="weight", amount=0.1, dim=0)
      prune.remove(module, 'weight')

  torch.save(model300rs.state_dict(), "quant.p")
  print('Size after random structured pruning LeNet300 (MB):', os.path.getsize("quant.p")/1e6)
  os.remove('quant.p')

  print('Top1 error on LeNet300 random structured: %f %%' % (model300rs.evaluate()))

  #post training quantization

  # model300quant.qconfig = torch.quantization.default_qconfig
  # torch.quantization.prepare(model300quant, inplace=True)
  # model300quant.evaluate()
  # torch.quantization.convert(model300quant, inplace=True)

  # # Convert pruned weights to CSR format
  # for name, module in model300quant.named_modules():
  #     if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):
  #         prune.remove(module, 'weight')
  #         module.weight = nn.Parameter(module.weight.to_sparse().coalesce())
