# -*- coding: utf-8 -*-
"""LeNetMNIST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_B3vXnQMHsrA4vFvytrQgWeLFdKVrJuS
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import torch.quantization
import torch.nn.utils.prune as prune
from torch.ao.quantization import QuantStub, DeQuantStub
import os

#LeNet5 network
class LeNet5(nn.Module):
    def __init__(self, q):
        self.q = q
        super(LeNet5, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc1 = nn.Linear(16 * 4 * 4, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)
        if self.q:
          self.quant = QuantStub()
          self.dequant = DeQuantStub()

    def forward(self, x):
        if self.q:
          x = self.quant(x)
        x = torch.relu(self.conv1(x))
        x = torch.max_pool2d(x, 2)
        x = torch.relu(self.conv2(x))
        x = torch.max_pool2d(x, 2)
        x = x.reshape(-1, 16 * 4 * 4)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        if self.q:
          x = self.dequant(x)
        return x

    def train(self):
      criterion = nn.CrossEntropyLoss()
      optimizer = optim.Adam(self.parameters(), lr=0.001)

      # Train the models
      for epoch in range(5):
          for i, (images, labels) in enumerate(train_loader):
              optimizer.zero_grad()
              outputs = self(images)
              loss = criterion(outputs, labels)
              loss.backward()
              optimizer.step()

    def evaluate(self):
      self.correct = 0
      self.total = 0
      with torch.no_grad():
          for inputs, labels in test_loader:
              outputs = self(inputs)
              _, predicted = torch.max(outputs.data, 1)
              self.total += labels.size(0)
              self.correct += (predicted == labels).sum().item()

#LeNet300 network
class LeNet300(nn.Module):
    def __init__(self, q):
        self.q = q
        super(LeNet300, self).__init__()
        self.fc1 = nn.Linear(28*28, 300)
        self.fc2 = nn.Linear(300, 100)
        self.fc3 = nn.Linear(100, 10)
        self.relu = nn.ReLU()
        if self.q:
          self.quant = QuantStub()
          self.dequant = DeQuantStub()

    def forward(self, x):
        if self.q:
          x = self.quant(x)
        x = x.reshape(-1, 28*28)  # Flatten the input images
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        if self.q:
          x = self.dequant(x)
        return x

    def train(self):
      criterion = nn.CrossEntropyLoss()
      optimizer = optim.Adam(self.parameters(), lr=0.001)
      total_step = len(train_loader)
      for epoch in range(10):
          for i, (images, labels) in enumerate(train_loader):
              outputs = self(images)
              loss = criterion(outputs, labels)
              optimizer.zero_grad()
              loss.backward()
              optimizer.step()

    def evaluate(self):
      self.correct = 0
      self.total = 0
      with torch.no_grad():
          for inputs, labels in test_loader:
              outputs = self(inputs)
              _, predicted = torch.max(outputs.data, 1)
              self.total += labels.size(0)
              self.correct += (predicted == labels).sum().item()

if __name__ == "__main__":
  #get dataset and normalize
  transform = transforms.Compose([
      transforms.ToTensor(),
      transforms.Normalize((0.1307,), (0.3081,))
  ])
  train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
  test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)
  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)
  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)

  #initialize models
  # model5quant = LeNet5(True)
  # model5noquant = LeNet5(False)

  model300quant = LeNet300(True)
  # model300noquant = LeNet300(False)

  #print model size without quantization
  # torch.save(model5noquant.state_dict(), "noquant.p")
  # print('Size no quantization LeNet5 (MB):', os.path.getsize("noquant.p")/1e6)
  # os.remove('noquant.p')

  # torch.save(model300noquant.state_dict(), "noquant.p")
  # print('Size no quantization LeNet300 (MB):', os.path.getsize("noquant.p")/1e6)
  # os.remove('noquant.p')

  #pretraining quantization
  # model5quant.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
  # torch.quantization.prepare_qat(model5quant, inplace=True)

  model300quant.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
  torch.quantization.prepare_qat(model300quant, inplace=True)

  #train models
  # model5quant.train()
  # model5noquant.train()

  model300quant.train()
  # model300noquant.train()

  # # Apply compression to remove small weights
  # prune.global_unstructured(
  #   model300quant.parameters,
  #   pruning_method=prune.L1Unstructured,
  #   amount=0.2,
  # )


  # for name, module in model300quant.named_modules():
  #   if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):
  #       module.weight = nn.Parameter(module.weight.to_sparse().coalesce())

  #post training quantization
  # torch.quantization.convert(model5quant, inplace=True)
  torch.quantization.convert(model300quant, inplace=True)

  for name, module in model300quant.named_modules():
    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):
      prune.random_unstructured(module, name="weight", amount=0.3)

  # Convert pruned weights to CSR format
  for name, module in model300quant.named_modules():
      if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):
          prune.remove(module, 'weight')
          module.weight = nn.Parameter(module.weight.to_sparse().coalesce())

  #print size after quantization
  # torch.save(model5quant.state_dict(), "quant.p")
  # print('Size quantized LeNet5 (MB):', os.path.getsize("quant.p")/1e6)
  # os.remove('quant.p')

  torch.save(model300quant.state_dict(), "quant.p")
  print('Size quantized LeNet300 (MB):', os.path.getsize("quant.p")/1e6)
  os.remove('quant.p')

  #evaluate models
  # model5quant.evaluate()
  # model5noquant.evaluate()

  model300quant.evaluate()
  # model300noquant.evaluate()

  #print results
  # print('Top1 error on LeNet5: %f %%' % (100 * (1 - (model5noquant.correct / model5noquant.total))))
  # print('Top1 error on LeNet5 quantized: %f %%' % (100 * (1 - (model5quant.correct / model5quant.total))))

  # print('Top1 error on LeNet300: %f %%' % (100 * (1 - (model300noquant.correct / model300noquant.total))))
  print('Top1 error on LeNet300 quantized: %f %%' % (100 * (1 - (model300quant.correct / model300quant.total))))
